{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_select_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_import = [\n",
    "    'utils',\n",
    "    'config_loader',\n",
    "    'base.pipeline.csv_loader',\n",
    "    'base.preprocessor.count_vectorizer',\n",
    "    'base.preprocessor.sklearn_preprocessor',\n",
    "    'base.pipeline.data_splitter',\n",
    "    'base.pipeline.mlp_term_selector',\n",
    "    'base.pipeline.data_splitter',\n",
    "    'base.pipeline.csv_saver'\n",
    "]\n",
    "\n",
    "import importer\n",
    "importer.import_modules(__name__, __file__, to_import)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger('pipeline')\n",
    "Config = config_loader.ConfigLoader(\n",
    "    'config.json'\n",
    ")\n",
    "\n",
    "with open(\"label_mapping.json\", encoding='utf-8') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "data_columns = ['TARGET','TEXT']\n",
    "term_selector_limit = 4000\n",
    "\n",
    "Config.label_mapping = label_mapping\n",
    "Config.data_columns = data_columns\n",
    "Config.term_selector_limit = term_selector_limit\n",
    "\n",
    "################################################################################\n",
    "\n",
    "data_loader1 = csv_loader.CSVLoader(**Config[\"csv_loader_params\"])\n",
    "cv1 = count_vectorizer.CV(step_name = 'cv1', **Config[\"cv_params\"])\n",
    "tfidf1 = sklearn_preprocessor.SklearnPreprocessor(step_name = 'tfidf1', **Config[\"tfidf_params\"])\n",
    "data_splitter1 = data_splitter.DataSplitter(**Config[\"splitter_params\"])\n",
    "term_selector = mlp_term_selector.MLPTermSelector(**Config[\"mlp_term_selector_params\"])\n",
    "data_loader2 = csv_loader.CSVLoader(**Config[\"csv_loader_params\"])\n",
    "cv2 = count_vectorizer.CV(step_name = 'cv2', **Config[\"cv_params\"])\n",
    "tfidf2 = sklearn_preprocessor.SklearnPreprocessor(step_name = 'tfidf2', **Config[\"tfidf_params\"])\n",
    "data_splitter2 = data_splitter.DataSplitter(**Config[\"splitter_params\"])\n",
    "csv_saver = csv_saver.CSVSaver(**Config[\"csv_saver_params\"])\n",
    "\n",
    "################################################################################\n",
    "\n",
    "@utils.catch('PIPELINE_RUNERROR')\n",
    "def run(debug = False):\n",
    "    x = {}\n",
    "    x = data_loader1(x, debug)\n",
    "    x = cv1.fit_transform(x, debug)\n",
    "    x = tfidf1.fit_transform(x, debug)\n",
    "    x = data_splitter1(x, debug)\n",
    "    x = term_selector(x, debug)\n",
    "    \n",
    "    x = {}\n",
    "    x = data_loader2(x, debug)\n",
    "    x = tfidf2.fit_transform(x, debug)\n",
    "    x = data_splitter2(x, debug)\n",
    "    csv_saver(x, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_import = [\n",
    "    'utils',\n",
    "    'config_loader',\n",
    "    'base.pipeline.csv_loader',\n",
    "    'base.preprocessor.sklearn_preprocessor',\n",
    "    'base.model.multi_model'\n",
    "]\n",
    "\n",
    "import importer\n",
    "importer.import_modules(__name__, __file__, to_import)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger('pipeline')\n",
    "Config = config_loader.ConfigLoader(\n",
    "    'config.json'\n",
    ")\n",
    "\n",
    "with open(\"label_mapping.json\", encoding='utf-8') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "model_type = 'mlp'\n",
    "term_selector_limit = 4000\n",
    "\n",
    "Config.label_mapping = label_mapping\n",
    "Config.data_columns = Config[\"data_columns\"]\n",
    "Config.term_selector_limit = term_selector_limit\n",
    "Config.model_type = model_type\n",
    "\n",
    "################################################################################\n",
    "\n",
    "data_loader = csv_loader.CSVLoader(**Config[\"csv_loader_params\"])\n",
    "model = multi_model.MultiModel(**Config[\"model_params\"])\n",
    "\n",
    "################################################################################\n",
    "\n",
    "@utils.catch('PIPELINE_RUNERROR')\n",
    "def run(debug = False):\n",
    "    x = {}\n",
    "    x = data_loader(x, debug)\n",
    "    \n",
    "    results = model.search(x, debug)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_import = [\n",
    "    'utils',\n",
    "    'config_loader',\n",
    "    'text_classification.retriever',\n",
    "    'base.preprocessor.count_vectorizer',\n",
    "    'base.preprocessor.sklearn_preprocessor',\n",
    "    'base.model.multi_model',\n",
    "    'text_classification.delivery'\n",
    "]\n",
    "\n",
    "import importer\n",
    "importer.import_modules(__name__, __file__, to_import)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger('pipeline')\n",
    "Config = config_loader.ConfigLoader(\n",
    "    'config.json'\n",
    ")\n",
    "\n",
    "with open(\"label_mapping.json\", encoding='utf-8') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "model_type = 'mlp'\n",
    "term_selector_limit = 4000\n",
    "\n",
    "Config.label_mapping = label_mapping\n",
    "Config.data_columns = Config[\"data_columns\"]\n",
    "Config.term_selector_limit = term_selector_limit\n",
    "Config.model_type = model_type\n",
    "\n",
    "################################################################################\n",
    "\n",
    "retriever = retriever.Retriever()\n",
    "cv2 = count_vectorizer.CV(step_name = 'cv2', **Config[\"cv_params\"])\n",
    "tfidf2 = sklearn_preprocessor.SklearnPreprocessor(step_name = 'tfidf2', **Config[\"tfidf_params\"])\n",
    "model = multi_model.MultiModel(**Config[\"model_params\"])\n",
    "delivery = delivery.Retriever()\n",
    "\n",
    "################################################################################\n",
    "\n",
    "@utils.catch('PIPELINE_RUNERROR')\n",
    "def predict(x = None, DocId = '', debug = False):\n",
    "    x = {}\n",
    "    \n",
    "    x = retriever(x)\n",
    "    x = cv2.transform(x, debug)\n",
    "    x = tfidf2.transform(x, debug)\n",
    "    x = model.predict(x, debug)\n",
    "    x = delivery(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_msg = \"\"\n",
    "predict(example_msg, 'ID012345')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
